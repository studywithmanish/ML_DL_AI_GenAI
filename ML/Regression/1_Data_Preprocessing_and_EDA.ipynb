{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Dataset: Data Preprocessing and Exploratory Data Analysis\n",
    "\n",
    "This notebook focuses on the initial data exploration, preprocessing, and exploratory data analysis of the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('bostonHousing.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check the shape of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Get column information\n",
    "print(\"\\nColumn information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical summary of the dataset:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Visualize missing values if any\n",
    "if missing_values.sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handling Outliers\n",
    "\n",
    "Let's identify outliers using box plots and the Interquartile Range (IQR) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create box plots for all numeric features\n",
    "plt.figure(figsize=(15, 10))\n",
    "df.boxplot()\n",
    "plt.title('Box Plots for All Features')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers for each numeric column\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "for column in numeric_columns:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, column)\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\nOutliers in {column}:\")\n",
    "        print(f\"Number of outliers: {len(outliers)}\")\n",
    "        print(f\"Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "        print(f\"Min value: {df[column].min():.2f}, Max value: {df[column].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import scaling libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Create a copy of the dataframe for scaling\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Separate target variable\n",
    "X = df_scaled.drop(['MEDV', 'CAT. MEDV'], axis=1)\n",
    "y = df_scaled['MEDV']\n",
    "\n",
    "# Apply different scaling techniques\n",
    "# 1. Standard Scaling (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard = scaler_standard.fit_transform(X)\n",
    "X_standard_df = pd.DataFrame(X_standard, columns=X.columns)\n",
    "\n",
    "# 2. Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "X_minmax_df = pd.DataFrame(X_minmax, columns=X.columns)\n",
    "\n",
    "# 3. Robust Scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "X_robust = scaler_robust.fit_transform(X)\n",
    "X_robust_df = pd.DataFrame(X_robust, columns=X.columns)\n",
    "\n",
    "# Compare the distributions after scaling\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 20))\n",
    "\n",
    "# Original data\n",
    "sns.boxplot(data=X, ax=axes[0])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xticklabels(X.columns, rotation=90)\n",
    "\n",
    "# Standard scaled data\n",
    "sns.boxplot(data=X_standard_df, ax=axes[1])\n",
    "axes[1].set_title('Standard Scaled Data')\n",
    "axes[1].set_xticklabels(X.columns, rotation=90)\n",
    "\n",
    "# Min-Max scaled data\n",
    "sns.boxplot(data=X_minmax_df, ax=axes[2])\n",
    "axes[2].set_title('Min-Max Scaled Data')\n",
    "axes[2].set_xticklabels(X.columns, rotation=90)\n",
    "\n",
    "# Robust scaled data\n",
    "sns.boxplot(data=X_robust_df, ax=axes[3])\n",
    "axes[3].set_title('Robust Scaled Data')\n",
    "axes[3].set_xticklabels(X.columns, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Engineering\n",
    "\n",
    "Let's create some new features that might be useful for our regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# 1. Log transformation for skewed features\n",
    "skewed_features = ['CRIM', 'ZN', 'DIS', 'LSTAT']\n",
    "for feature in skewed_features:\n",
    "    df_fe[f'{feature}_log'] = np.log1p(df_fe[feature])\n",
    "\n",
    "# 2. Polynomial features for important predictors\n",
    "poly_features = ['RM', 'LSTAT', 'DIS']\n",
    "for feature in poly_features:\n",
    "    df_fe[f'{feature}_squared'] = df_fe[feature] ** 2\n",
    "\n",
    "# 3. Interaction terms\n",
    "df_fe['RM_LSTAT'] = df_fe['RM'] * df_fe['LSTAT']\n",
    "df_fe['NOX_INDUS'] = df_fe['NOX'] * df_fe['INDUS']\n",
    "df_fe['RM_AGE'] = df_fe['RM'] / (df_fe['AGE'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "# Display the new features\n",
    "print(\"New features created:\")\n",
    "new_features = [col for col in df_fe.columns if col not in df.columns]\n",
    "df_fe[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Distribution of Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of the target variable (MEDV)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['MEDV'], kde=True)\n",
    "plt.title('Distribution of MEDV (Median Home Value)')\n",
    "plt.xlabel('MEDV ($1000s)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['MEDV'])\n",
    "plt.title('Box Plot of MEDV')\n",
    "plt.ylabel('MEDV ($1000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for skewness in the target variable\n",
    "skewness = df['MEDV'].skew()\n",
    "kurtosis = df['MEDV'].kurt()\n",
    "\n",
    "print(f\"Skewness of MEDV: {skewness:.4f}\")\n",
    "print(f\"Kurtosis of MEDV: {kurtosis:.4f}\")\n",
    "\n",
    "# If skewness is significant, apply log transformation\n",
    "if abs(skewness) > 0.5:\n",
    "    df['MEDV_log'] = np.log1p(df['MEDV'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df['MEDV'], kde=True)\n",
    "    plt.title('Original MEDV Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(df['MEDV_log'], kde=True)\n",
    "    plt.title('Log-Transformed MEDV Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Skewness after log transformation: {df['MEDV_log'].skew():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot heatmap of correlations\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Boston Housing Features')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation with target variable\n",
    "target_corr = correlation_matrix['MEDV'].sort_values(ascending=False)\n",
    "print(\"Correlation with MEDV (target variable):\")\n",
    "print(target_corr)\n",
    "\n",
    "# Visualize top correlations with target\n",
    "plt.figure(figsize=(12, 8))\n",
    "target_corr.drop('MEDV').plot(kind='bar')\n",
    "plt.title('Feature Correlation with MEDV')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Multicollinearity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for multicollinearity using Variance Inflation Factor (VIF)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create a dataframe for VIF calculation\n",
    "X_vif = df.drop(['MEDV', 'CAT. MEDV'], axis=1)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# Sort by VIF value\n",
    "vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "print(\"Variance Inflation Factor (VIF) for features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Visualize VIF values\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=\"VIF\", y=\"Feature\", data=vif_data)\n",
    "plt.title('Variance Inflation Factor (VIF) for Features')\n",
    "plt.axvline(x=5, color='red', linestyle='--', label='VIF=5 (Threshold for high multicollinearity)')\n",
    "plt.axvline(x=10, color='darkred', linestyle='--', label='VIF=10 (Threshold for severe multicollinearity)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the distribution of each feature\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(df.drop(['MEDV', 'CAT. MEDV'], axis=1).columns):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scatter plots of top correlated features with MEDV\n",
    "top_corr_features = target_corr.drop('MEDV').abs().sort_values(ascending=False).head(4).index\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(top_corr_features):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.scatterplot(x=df[feature], y=df['MEDV'])\n",
    "    plt.title(f'MEDV vs {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('MEDV')\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=df[feature], y=df['MEDV'], scatter=False, color='red')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Pair Plots for Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select important features based on correlation\n",
    "important_features = list(top_corr_features) + ['MEDV']\n",
    "subset_df = df[important_features]\n",
    "\n",
    "# Create pair plot\n",
    "sns.pairplot(subset_df, diag_kind='kde')\n",
    "plt.suptitle('Pair Plot of Key Features', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary of Findings\n",
    "\n",
    "### Data Preprocessing:\n",
    "1. **Missing Values**: We checked for missing values in the dataset.\n",
    "2. **Outliers**: We identified outliers using box plots and the IQR method.\n",
    "3. **Feature Scaling**: We applied different scaling techniques (Standard, MinMax, Robust) to normalize the features.\n",
    "4. **Feature Engineering**: We created new features through log transformations, polynomial features, and interaction terms.\n",
    "\n",
    "### Exploratory Data Analysis:\n",
    "1. **Target Variable Distribution**: We analyzed the distribution of the target variable (MEDV) and applied transformations if needed.\n",
    "2. **Correlation Analysis**: We identified the features most correlated with the target variable.\n",
    "3. **Multicollinearity**: We checked for multicollinearity among features using VIF.\n",
    "4. **Feature Visualization**: We visualized the distribution of features and their relationships with the target variable.\n",
    "\n",
    "### Key Insights:\n",
    "- The most important features for predicting median home values appear to be [will be filled based on actual analysis].\n",
    "- Some features show high multicollinearity, which might affect model performance.\n",
    "- The target variable distribution shows [will be filled based on actual analysis].\n",
    "- Feature engineering, especially [will be filled based on actual analysis], might improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
